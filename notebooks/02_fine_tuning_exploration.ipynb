{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e0cdcf8-450a-43dc-9899-7510d2ccd9a5",
   "metadata": {},
   "source": [
    "# 2. Fine Tuning Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "002394d5-038d-4062-8434-2a9b636ad0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-30 01:07:03.521778: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-30 01:07:04.056894: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from master_thesis import ModelFactory, DatasetFactory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ead193-7220-4f2a-b6e5-1d83d7f8eabe",
   "metadata": {},
   "source": [
    "## 2.1 Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89fbe229-caca-4878-b8cc-6bc94c98f210",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_factory = ModelFactory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4b9c7e5-f1ed-49ca-afdc-b80befb51216",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomdewildt/Projects/master-thesis/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/tomdewildt/Projects/master-thesis/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = model_factory.get_model(\"openai/gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c3e8ce-9137-41c5-9509-fd7054cfc26f",
   "metadata": {},
   "source": [
    "## 2.2 Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b8fb9cf-10c1-4366-964b-77b7910408c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_factory = DatasetFactory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6279bde4-5529-410a-8d4b-087b75297fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_factory.get_dataset(\"coqa\", train_limit=100, test_limit=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4637a43a-890b-473f-a3ef-f1af33ff0d3b",
   "metadata": {},
   "source": [
    "## 2.3 Finetune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42581e98-86c7-40cc-aa6b-693fa89efd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomdewildt/Projects/master-thesis/venv/lib/python3.11/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n",
      "/home/tomdewildt/Projects/master-thesis/venv/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:166: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:30, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.443900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/tomdewildt/Projects/master-thesis/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<HuggingFaceAutoregressiveTextModel(model_id=openai/gpt2, max_tokens=256, stop_sequences=None, temperature=0.7, top_p=1.0)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.finetune(dataset, lambda references, question, answer: references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248007b1-bb07-4fc1-ab73-cb0bd2f85b64",
   "metadata": {},
   "source": [
    "## 2.4 Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88943538-5adb-4965-bef4-cfada63b585f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The capital of Italy is Rome. Rome is the capital of Italy and the city center of Italy is Rome. The city of Rome is located at the southern end of the Mediterranean Sea. It is not on the northern coast of Italy but is located on the northern coast of the Mediterranean. There is an island at San Marzocchi, in the south of Italy, called Rome. This island is located in the center of the Mediterranean Sea. It is located in the center of the Mediterranean Sea. It is not on the northern coast of Italy but is located on the northern coast of the Mediterranean.\\nThe city of Rome is located on the northern coast of the Mediterranean. It is located in the center of the Mediterranean Sea. It is not on the northern coast of Italy but is located on the northern coast of the Mediterranean.\\nThe city of Rome is located on the northern coast of the Mediterranean. It is located on the northern coast of the Mediterranean. It is not on the northern coast of Italy but is located on the northern coast of the Mediterranean.\\nA large island is located on the northern coast of Italy. It is located on the northern coast of Italy. It is located on the northern coast of the'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(\"Q: What is the capital of Italy?\\nA:\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
