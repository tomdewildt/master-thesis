{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79c1fb79-2e66-46e9-92ec-eb014928196d",
   "metadata": {},
   "source": [
    "# 3. Prompt Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b8eed9b-950f-4aec-a555-4be23fb4f433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q accelerate anthropic google-cloud-aiplatform langchain openai protobuf sentencepiece torch transformers xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ab9b527-02c2-4ef8-bc01-f6d2688191d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from master_thesis.base import BaseModel\n",
    "from master_thesis.examples import Example\n",
    "from master_thesis.factories import ModelFactory, PromptFactory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f788e3-2aff-43bf-9a82-7f237a325291",
   "metadata": {},
   "source": [
    "## 3.1 Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d59f3f8e-1a4b-4814-98cd-56a9256a9825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['few-shot-cot',\n",
       " 'few-shot',\n",
       " 'instruction',\n",
       " 'least-to-most',\n",
       " 'plan-and-solve',\n",
       " 'self-ask',\n",
       " 'self-consistency',\n",
       " 'zero-shot-cot',\n",
       " 'zero-shot']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factory = ModelFactory()\n",
    "model = factory.get_model(\"openai/gpt-3.5-turbo\")\n",
    "\n",
    "factory = PromptFactory()\n",
    "factory.list_prompts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5195922-8722-47f5-a54c-3a72d73c61e2",
   "metadata": {},
   "source": [
    "## 3.2 Evaluate Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8af62c-e26f-4ae1-9e74-138c65e32bcb",
   "metadata": {},
   "source": [
    "### 3.2.1 Zero Shot Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd29367d-9deb-47c3-bd98-62ec20e4c601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There is no correct letter mentioned in the given information.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = factory.get_prompt(\"zero-shot\", model)\n",
    "prompt.run([\"a\", \"b\", \"c\"], \"What is the correct letter?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1de05a7-ea1b-4b9b-a45d-80f2d366f9e0",
   "metadata": {},
   "source": [
    "### 3.2.2 Few Shot Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2b3998a-71f3-4ca2-9387-ca756333202a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = factory.get_prompt(\"few-shot\", model)\n",
    "prompt.run(\n",
    "    [\"a\", \"b\", \"c\"],\n",
    "    \"What is the correct letter?\",\n",
    "    examples=[\n",
    "        Example(\n",
    "            references=[\"d\", \"e\", \"f\"],\n",
    "            question=\"What is the correct letter?\",\n",
    "            answer=\"d.\",\n",
    "        ),\n",
    "        Example(\n",
    "            references=[\"g\", \"h\", \"i\"],\n",
    "            question=\"What is the correct letter?\",\n",
    "            answer=\"g.\",\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28af67cd-6b77-4c44-8bca-efb63d3a983b",
   "metadata": {},
   "source": [
    "### 3.2.3 Instruction Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8084c5d7-3c40-424d-9f46-0b899f991ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = factory.get_prompt(\"instruction\", model)\n",
    "prompt.run(\n",
    "    [\"a\", \"b\", \"c\"],\n",
    "    \"What is the correct letter?\",\n",
    "    instruction=\"Given the context below answer the question. The first element is always correct.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d471a88-4b69-4da2-9c50-ea4fa614753f",
   "metadata": {},
   "source": [
    "### 3.2.4 Zero Shot Chain-of-Thought Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3edace6d-d254-4614-926b-896d79b0631f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The correct letter is \"c\".'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = factory.get_prompt(\"zero-shot-cot\", model)\n",
    "prompt.run([\"a\", \"b\", \"c\"], \"What is the correct letter?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cab2742-a4c4-4268-8b30-dd5eefb4f07d",
   "metadata": {},
   "source": [
    "### 3.2.5 Few Shot Chain-of-Thought Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b660ae5-7c0c-47d4-9389-ba164298b8ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The correct letter is c.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = factory.get_prompt(\"few-shot-cot\", model)\n",
    "prompt.run(\n",
    "    [\"a\", \"b\", \"c\"],\n",
    "    \"What is the correct letter?\",\n",
    "    examples=[\n",
    "        Example(\n",
    "            references=[\"d\", \"e\", \"f\"],\n",
    "            question=\"What is the correct letter?\",\n",
    "            answer=\"Since no further context is given we can only guess to correct letter. We guess the correct letter is d.\",\n",
    "        ),\n",
    "        Example(\n",
    "            references=[\"g\", \"h\", \"i\"],\n",
    "            question=\"What is the correct letter?\",\n",
    "            answer=\"We cannot deduce which letter is the correct one since it is not provided in the context. We guess the correct letter is g.\",\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd750659-4e0b-47d7-9753-d90d2b5456bf",
   "metadata": {},
   "source": [
    "### 3.2.6 Self-consistency Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b799103a-e6c9-4095-8ecb-d91819f606d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The correct letter is c.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = factory.get_prompt(\"self-consistency\", model)\n",
    "prompt.run(\n",
    "    [\"a\", \"b\", \"c\"],\n",
    "    \"What is the correct letter?\",\n",
    "    n=6,\n",
    "    examples=[\n",
    "        Example(\n",
    "            references=[\"d\", \"e\", \"f\"],\n",
    "            question=\"What is the correct letter?\",\n",
    "            answer=\"Since no further context is given we can only guess to correct letter. We guess the correct letter is d.\",\n",
    "        ),\n",
    "        Example(\n",
    "            references=[\"g\", \"h\", \"i\"],\n",
    "            question=\"What is the correct letter?\",\n",
    "            answer=\"We cannot deduce which letter is the correct one since it is not provided in the context. We guess the correct letter is g.\",\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4523bba-1b11-454e-9169-ed7d6d284196",
   "metadata": {},
   "source": [
    "### 3.2.7 Self-ask Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29e04927-da99-4877-854b-c392b9069b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = factory.get_prompt(\"self-ask\", model)\n",
    "prompt.run([\"a\", \"b\", \"c\"], \"What is the correct letter?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc146df2-fd9e-4bb4-afb9-fd4d6d31f54a",
   "metadata": {},
   "source": [
    "### 3.2.8 Least-to-Most Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33e6475a-2dee-4e90-963d-4aac3c20a27a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. What are the options for the correct letter? 2. How can we determine the correct letter?\\n1. The options for the correct letter are a, b, and c.\\n2. To determine the correct letter, we would need more information or context. Without any additional information, we cannot determine the correct letter.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = factory.get_prompt(\"least-to-most\", model)\n",
    "prompt.run([\"a\", \"b\", \"c\"], \"What is the correct letter?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ad9e6c-bb6a-4793-b4af-3da1b0bde1fe",
   "metadata": {},
   "source": [
    "### 3.2.9 Plan-and-Solve Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "214a07be-4928-432a-88fd-0638ecc7ab6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It seems that the given information does not provide any criteria to determine the correct letter. Please provide more context or clarification to help us find the correct letter.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = factory.get_prompt(\"plan-and-solve\", model)\n",
    "prompt.run([\"a\", \"b\", \"c\"], \"What is the correct letter?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
