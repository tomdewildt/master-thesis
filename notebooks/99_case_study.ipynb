{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f755b03-8a54-497a-be23-1bb1601031fb",
   "metadata": {},
   "source": [
    "# 99. Case Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20843064-857c-4f0b-b7f4-046c5679b8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List, Optional\n",
    "\n",
    "from datasets import Dataset\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.evaluation import EvaluatorType, load_evaluator\n",
    "\n",
    "from master_thesis.base import BaseMetric, BasePrompt\n",
    "from master_thesis.datasets import SQUADv1Dataset\n",
    "from master_thesis.defaults import DEFAULT_PEFT_CONFIG, DEFAULT_TRAIN_CONFIG, RANDOM_SEED\n",
    "from master_thesis.experiments import Experiment\n",
    "from master_thesis.utils import format_references, format_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e05507",
   "metadata": {},
   "source": [
    "## 99.1 Create Custom Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dee2c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLLaMAPrompt(BasePrompt):\n",
    "    def run(self, references: Optional[List[str]], question: str) -> str:\n",
    "        prompt = f\"\"\"<s>[INST] <<SYS>>Context:\n",
    "        {self._format_references(references)}\n",
    "        <</SYS>>\n",
    "\n",
    "        Q: {question} [/INST]\n",
    "        A: \n",
    "        \"\"\"\n",
    "\n",
    "        return self._model.generate(self._format_prompt(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f301ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoContextDataset(SQUADv1Dataset):\n",
    "    _percent_no_context: float\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        percent_no_context: int = 0.10,\n",
    "        train_limit: Optional[int] = None,\n",
    "        test_limit: Optional[int] = None,\n",
    "    ) -> None:\n",
    "        super().__init__(train_limit=train_limit, test_limit=test_limit)\n",
    "\n",
    "        self._percent_no_context = percent_no_context\n",
    "\n",
    "    @property\n",
    "    def train(self) -> Dataset:\n",
    "        if not self._train:\n",
    "            self._train = (\n",
    "                self._dataset_dict[\"train\"]\n",
    "                .map(\n",
    "                    self._format,\n",
    "                    batched=True,\n",
    "                    remove_columns=self._dataset_dict[\"train\"].column_names,\n",
    "                )\n",
    "                .shuffle(seed=RANDOM_SEED)\n",
    "            )\n",
    "\n",
    "            # Add no context samples\n",
    "            self._train = self._train.map(\n",
    "                self._introduce_no_context_samples(\n",
    "                    0,\n",
    "                    self._train.num_rows * self._percent_no_context,\n",
    "                ),\n",
    "                batched=False,\n",
    "                with_indices=True,\n",
    "            ).shuffle(seed=RANDOM_SEED)\n",
    "\n",
    "            if self._train_limit:\n",
    "                self._train = self._train.select(range(self._train_limit))\n",
    "\n",
    "        return self._train\n",
    "\n",
    "    @property\n",
    "    def test(self) -> Dataset:\n",
    "        if not self._test:\n",
    "            self._test = (\n",
    "                self._dataset_dict[\"validation\"]\n",
    "                .map(\n",
    "                    self._format,\n",
    "                    batched=True,\n",
    "                    remove_columns=self._dataset_dict[\"validation\"].column_names,\n",
    "                )\n",
    "                .shuffle(seed=RANDOM_SEED)\n",
    "            )\n",
    "\n",
    "            # Add no context samples\n",
    "            self._test = self._test.map(\n",
    "                self._introduce_no_context_samples(\n",
    "                    0,\n",
    "                    self._test.num_rows * self._percent_no_context,\n",
    "                ),\n",
    "                batched=False,\n",
    "                with_indices=True,\n",
    "            ).shuffle(seed=RANDOM_SEED)\n",
    "\n",
    "            if self._test_limit:\n",
    "                self._test = self._test.select(range(self._test_limit))\n",
    "\n",
    "        return self._test\n",
    "\n",
    "    def _introduce_no_context_samples(\n",
    "        self,\n",
    "        start_idx: int,\n",
    "        end_idx: int,\n",
    "    ) -> Callable[[Dict[str, Any], int], Dict[str, Any]]:\n",
    "        def map_function(sample: Dict[str, Any], idx: int) -> Dict[str, Any]:\n",
    "            if idx >= start_idx and idx <= end_idx:\n",
    "                return {**sample, \"references\": [\"\"], \"answer\": \"-\"}\n",
    "            return sample\n",
    "\n",
    "        return map_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a558e9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LevenshteinSimilarityMetric(BaseMetric):\n",
    "    _evaluator: Chain\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self._evaluator = load_evaluator(EvaluatorType.STRING_DISTANCE)\n",
    "\n",
    "    def test(self, question: str, answer: str, prediction: str) -> float:\n",
    "        return 1 - self._evaluator.evaluate_strings(\n",
    "            prediction=prediction,\n",
    "            reference=answer,\n",
    "        )[\"score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bc6a88",
   "metadata": {},
   "source": [
    "## 99.2 Test Base Model + Simple Prompt\n",
    "\n",
    "* Model: `meta-llama/llama-2-7b-chat-hf`\n",
    "* Finetune: `None`\n",
    "* Prompt: `SimpleLLaMAPrompt`\n",
    "* Dataset: `NoContextDataset`\n",
    "* Metric: `LevenshteinSimilarityMetric`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b110892",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_experiment = Experiment(\n",
    "    experiment_name=\"case_study_base_model_simple_prompt\",\n",
    "    model=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    model_config={\n",
    "        \"max_tokens\": 1024,\n",
    "        \"stop_sequences\": [\"\\n\"],\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 1.0,\n",
    "    },\n",
    "    dataset=NoContextDataset,\n",
    "    dataset_config={\"test_limit\": 1000},\n",
    "    metric=LevenshteinSimilarityMetric,\n",
    "    metric_config={},\n",
    "    prompt=SimpleLLaMAPrompt,\n",
    "    prompt_config={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49573274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model_experiment.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76b500c",
   "metadata": {},
   "source": [
    "## 99.3 Test Finetuned Model + Simple Prompt\n",
    "\n",
    "* Model: `meta-llama/llama-2-7b-chat-hf`\n",
    "* Finetune: `NoContextDataset`\n",
    "* Prompt: `SimpleLLaMAPrompt`\n",
    "* Dataset: `NoContextDataset`\n",
    "* Metric: `LevenshteinSimilarityMetric`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7ab961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_format_function(references: List[str], question: str, answer: str) -> str:\n",
    "        prompt = f\"\"\"<s>[INST] <<SYS>>Context:\n",
    "        {format_references(references)}\n",
    "        <</SYS>>\n",
    "\n",
    "        Q: {question} [/INST]\n",
    "        A: {answer}\n",
    "        \"\"\"\n",
    "\n",
    "        return format_prompt(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265de4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model_experiment = Experiment(\n",
    "    experiment_name=\"case_study_finetuned_model_simple_prompt\",\n",
    "    model=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    model_config={\n",
    "        \"max_tokens\": 1024,\n",
    "        \"stop_sequences\": [\"\\n\"],\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 1.0,\n",
    "    },\n",
    "    dataset=NoContextDataset,\n",
    "    dataset_config={\"test_limit\": 1000},\n",
    "    metric=LevenshteinSimilarityMetric,\n",
    "    metric_config={},\n",
    "    finetune_dataset=NoContextDataset,\n",
    "    finetune_format_function=finetune_format_function,\n",
    "    finetune_config={\n",
    "        \"dataset_config\": {\"train_limit\": 2000},\n",
    "        \"peft_config\": DEFAULT_PEFT_CONFIG,\n",
    "        \"train_config\": DEFAULT_TRAIN_CONFIG,\n",
    "    },\n",
    "    prompt=SimpleLLaMAPrompt,\n",
    "    prompt_config={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d27ed7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetuned_model_experiment.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d79ebe5",
   "metadata": {},
   "source": [
    "## 99.4  Test Base Model + Advanced Prompt\n",
    "\n",
    "* Model: `meta-llama/llama-2-7b-chat-hf`\n",
    "* Finetune: `None`\n",
    "* Prompt: `AdvancedLLaMAPrompt`\n",
    "* Dataset: `NoContextDataset`\n",
    "* Metric: `LevenshteinSimilarityMetric`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9640cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedLLaMAPrompt(BasePrompt):\n",
    "    def run(self, references: Optional[List[str]], question: str) -> str:\n",
    "        prompt = f\"\"\"<s>[INST] <<SYS>>Answer the question based on the context below:\n",
    "        * Only provide the answer, no extra explanation.\n",
    "        * Only use words from the context below in the answer.\n",
    "        * If the question cannot be answered using the context or if the context is missing, answer with '-'.\n",
    "        \n",
    "        -----\n",
    "        {self._format_references(references)}\n",
    "        <</SYS>>\n",
    "\n",
    "        Q: {question} [/INST]\n",
    "        A: \n",
    "        \"\"\"\n",
    "\n",
    "        return self._model.generate(self._format_prompt(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a20998c",
   "metadata": {},
   "outputs": [],
   "source": [
    "advanced_prompt_experiment = Experiment(\n",
    "    experiment_name=\"case_study_base_model_advanced_prompt\",\n",
    "    model=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    model_config={\n",
    "        \"max_tokens\": 1024,\n",
    "        \"stop_sequences\": [\"\\n\"],\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 1.0,\n",
    "    },\n",
    "    dataset=NoContextDataset,\n",
    "    dataset_config={\"test_limit\": 1000},\n",
    "    metric=LevenshteinSimilarityMetric,\n",
    "    metric_config={},\n",
    "    prompt=AdvancedLLaMAPrompt,\n",
    "    prompt_config={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae569dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# advanced_prompt_experiment.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
