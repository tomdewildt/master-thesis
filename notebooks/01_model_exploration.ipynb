{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e69bd2ff-ed57-4599-bedc-27e7345b58c8",
   "metadata": {},
   "source": [
    "# 1. Model Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ce9b939-32b1-4509-9d99-357e25e94b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q accelerate anthropic google-cloud-aiplatform langchain openai protobuf sentencepiece torch transformers xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f3ddfd9-1c58-438a-9200-4ed7b49874eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List\n",
    "\n",
    "from langchain.llms import HuggingFacePipeline, OpenAI, VertexAI\n",
    "from langchain.chat_models import ChatAnthropic, ChatOpenAI, ChatVertexAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8d42af7b-eab0-46ca-bc8b-d9646ba45285",
   "metadata": {},
   "source": [
    "## 1.1 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9abc9642-102a-40b6-ad7a-7a5b2629ddd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANTHROPIC_MODEL_IDS = [\n",
    "    \"anthropic/claude-v1\",\n",
    "    \"anthropic/claude-v1-100k\",\n",
    "    \"anthropic/claude-instant-v1\",\n",
    "    \"anthropic/claude-instant-v1-100k\",\n",
    "]\n",
    "\n",
    "GOOGLE_MODEL_IDS = [\n",
    "    \"google/text-bison-001\",\n",
    "    \"google/chat-bison-001\",\n",
    "]\n",
    "\n",
    "HUGGINGFACE_ENCODER_DECODER_MODEL_IDS = [\n",
    "    \"google/t5-small\",\n",
    "    \"google/t5-base\",\n",
    "    \"google/t5-large\",\n",
    "    \"google/t5-3b\",\n",
    "    \"google/t5-11b\",\n",
    "    \"google/flan-t5-small\",\n",
    "    \"google/flan-t5-base\",\n",
    "    \"google/flan-t5-large\",\n",
    "    \"google/flan-t5-xl\",\n",
    "    \"google/flan-t5-xxl\",\n",
    "]\n",
    "\n",
    "HUGGINGFACE_DECODER_MODEL_IDS = [\n",
    "    \"openai/gpt2\",\n",
    "    \"bigscience/bloom-560m\",\n",
    "    \"bigscience/bloom-1b1\",\n",
    "    \"bigscience/bloom-1b7\",\n",
    "    \"bigscience/bloom-3b\",\n",
    "    \"bigscience/bloom-7b1\",\n",
    "    \"bigscience/bloom\",\n",
    "    \"bigscience/bloomz-560m\",\n",
    "    \"bigscience/bloomz-1b1\",\n",
    "    \"bigscience/bloomz-1b7\",\n",
    "    \"bigscience/bloomz-3b\",\n",
    "    \"bigscience/bloomz-7b1\",\n",
    "    \"bigscience/bloomz\",\n",
    "    \"facebook/xglm-564M\",\n",
    "    \"facebook/xglm-1.7B\",\n",
    "    \"facebook/xglm-2.9B\",\n",
    "    \"facebook/xglm-4.5B\",\n",
    "    \"facebook/xglm-7.5B\",\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"meta-llama/Llama-2-13b-hf\",\n",
    "    \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "    \"meta-llama/Llama-2-70b-hf\",\n",
    "    \"meta-llama/Llama-2-70b-chat-hf\",\n",
    "]\n",
    "\n",
    "OPENAI_MODEL_IDS = [\n",
    "    \"openai/gpt-4\",\n",
    "    \"openai/gpt-3.5-turbo\",\n",
    "    \"openai/text-davinci-003\",\n",
    "    \"openai/text-curie-001\",\n",
    "    \"openai/text-babbage-001\",\n",
    "    \"openai/text-ada-001\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54436964-fab0-494e-a033-8c48926fb68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(ABC):\n",
    "    _model_id: str\n",
    "    _max_tokens: int\n",
    "    _stop_sequences: List[str]\n",
    "    _temperature: float\n",
    "    _top_p: float\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id: str,\n",
    "        max_tokens: int,\n",
    "        stop_sequences: List[str],\n",
    "        temperature: float,\n",
    "        top_p: float,\n",
    "    ):\n",
    "        self._model_id = model_id\n",
    "        self._max_tokens = max_tokens\n",
    "        self._stop_sequences = stop_sequences\n",
    "        self._temperature = temperature\n",
    "        self._top_p = top_p\n",
    "\n",
    "    @abstractmethod\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        pass\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"<{self.__class__.__name__}(model_id={self._model_id}, max_tokens={self._max_tokens}, stop_sequences={self._stop_sequences}, temperature={self._temperature}, top_p={self._top_p})>\"\n",
    "\n",
    "\n",
    "class AnthropicModel(BaseModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id: str,\n",
    "        max_tokens: int = 256,\n",
    "        stop_sequences: List[str] = None,\n",
    "        temperature: float = 0.7,\n",
    "        top_p: float = 1.0,\n",
    "    ):\n",
    "        super().__init__(model_id, max_tokens, stop_sequences, temperature, top_p)\n",
    "\n",
    "        self._model = ChatAnthropic(\n",
    "            model=model_id.split(\"/\")[1],\n",
    "            max_tokens_to_sample=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            anthropic_api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        output = self._model([HumanMessage(content=prompt)], stop=self._stop_sequences)\n",
    "\n",
    "        return output.content\n",
    "\n",
    "\n",
    "class GoogleModel(BaseModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id: str,\n",
    "        max_tokens: int = 256,\n",
    "        stop_sequences: List[str] = None,\n",
    "        temperature: float = 0.7,\n",
    "        top_p: float = 1.0,\n",
    "    ):\n",
    "        super().__init__(model_id, max_tokens, stop_sequences, temperature, top_p)\n",
    "\n",
    "        self._is_chat = model_id in [\"chat-bison-001\"]\n",
    "\n",
    "        if self._is_chat:\n",
    "            self._model = ChatVertexAI(\n",
    "                model=model_id.split(\"/\")[1],\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                credentials=None,\n",
    "                verbose=True,\n",
    "            )\n",
    "        else:\n",
    "            self._model = VertexAI(\n",
    "                model=model_id.split(\"/\")[1],\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                credentials=None,\n",
    "                verbose=True,\n",
    "            )\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        if self._is_chat:\n",
    "            output = self._model(\n",
    "                [HumanMessage(content=prompt)],\n",
    "                stop=self._stop_sequences,\n",
    "            )\n",
    "\n",
    "            return output.content\n",
    "        else:\n",
    "            output = self._model(prompt, stop=self._stop_sequences)\n",
    "\n",
    "            return output\n",
    "\n",
    "\n",
    "class HuggingFaceEncoderDecoderModel(BaseModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id: str,\n",
    "        max_tokens: int = 256,\n",
    "        stop_sequences: List[str] = None,\n",
    "        temperature: float = 0.7,\n",
    "        top_p: float = 1.0,\n",
    "    ):\n",
    "        super().__init__(model_id, max_tokens, stop_sequences, temperature, top_p)\n",
    "\n",
    "        self._hf_model_id = (\n",
    "            model_id.split(\"/\")[1]\n",
    "            if any([x in model_id for x in [\"openai/gpt2\", \"google/t5\"]])\n",
    "            else model_id\n",
    "        )\n",
    "\n",
    "        self._model = HuggingFacePipeline(\n",
    "            pipeline=pipeline(\n",
    "                \"text2text-generation\",\n",
    "                model=AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                    self._hf_model_id,\n",
    "                    device_map=\"auto\",\n",
    "                    torch_dtype=\"auto\",\n",
    "                    use_auth_token=os.getenv(\"HUGGINGFACE_API_KEY\"),\n",
    "                ),\n",
    "                tokenizer=AutoTokenizer.from_pretrained(\n",
    "                    self._hf_model_id,\n",
    "                    device_map=\"auto\",\n",
    "                    torch_dtype=\"auto\",\n",
    "                    use_auth_token=os.getenv(\"HUGGINGFACE_API_KEY\"),\n",
    "                ),\n",
    "                device_map=\"auto\",\n",
    "                max_length=max_tokens,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "            ),\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        output = self._model(prompt, stop=self._stop_sequences)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class HuggingFaceDecoderModel(BaseModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id: str,\n",
    "        max_tokens: int = 256,\n",
    "        stop_sequences: List[str] = None,\n",
    "        temperature: float = 0.7,\n",
    "        top_p: float = 1.0,\n",
    "    ):\n",
    "        super().__init__(model_id, max_tokens, stop_sequences, temperature, top_p)\n",
    "\n",
    "        self._hf_model_id = (\n",
    "            model_id.split(\"/\")[1]\n",
    "            if any([x in model_id for x in [\"openai/gpt2\", \"google/t5\"]])\n",
    "            else model_id\n",
    "        )\n",
    "\n",
    "        self._model = HuggingFacePipeline(\n",
    "            pipeline=pipeline(\n",
    "                \"text-generation\",\n",
    "                model=AutoModelForCausalLM.from_pretrained(\n",
    "                    self._hf_model_id,\n",
    "                    device_map=\"auto\",\n",
    "                    torch_dtype=\"auto\",\n",
    "                    use_auth_token=os.getenv(\"HUGGINGFACE_API_KEY\"),\n",
    "                ),\n",
    "                tokenizer=AutoTokenizer.from_pretrained(\n",
    "                    self._hf_model_id,\n",
    "                    device_map=\"auto\",\n",
    "                    torch_dtype=\"auto\",\n",
    "                    use_auth_token=os.getenv(\"HUGGINGFACE_API_KEY\"),\n",
    "                ),\n",
    "                device_map=\"auto\",\n",
    "                max_new_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "            ),\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        output = self._model(prompt, stop=self._stop_sequences)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class OpenAIModel(BaseModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id: str,\n",
    "        max_tokens: int = 256,\n",
    "        stop_sequences: List[str] = None,\n",
    "        temperature: float = 0.7,\n",
    "        top_p: float = 1.0,\n",
    "    ):\n",
    "        super().__init__(model_id, max_tokens, stop_sequences, temperature, top_p)\n",
    "\n",
    "        self._is_chat = model_id in [\"openai/gpt-4\", \"openai/gpt-3.5-turbo\"]\n",
    "\n",
    "        if self._is_chat:\n",
    "            self._model = ChatOpenAI(\n",
    "                model=model_id.split(\"/\")[1],\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "                model_kwargs={\"top_p\": top_p},\n",
    "                openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "                verbose=True,\n",
    "            )\n",
    "        else:\n",
    "            self._model = OpenAI(\n",
    "                model=model_id.split(\"/\")[1],\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "                verbose=True,\n",
    "            )\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        if self._is_chat:\n",
    "            output = self._model(\n",
    "                [HumanMessage(content=prompt)],\n",
    "                stop=self._stop_sequences,\n",
    "            )\n",
    "\n",
    "            return output.content\n",
    "        else:\n",
    "            output = self._model(prompt, stop=self._stop_sequences)\n",
    "\n",
    "            return output\n",
    "\n",
    "\n",
    "class ModelFactory:\n",
    "    def list_models(self) -> List[str]:\n",
    "        return (\n",
    "            ANTHROPIC_MODEL_IDS\n",
    "            + GOOGLE_MODEL_IDS\n",
    "            + HUGGINGFACE_ENCODER_DECODER_MODEL_IDS\n",
    "            + HUGGINGFACE_DECODER_MODEL_IDS\n",
    "            + OPENAI_MODEL_IDS\n",
    "        )\n",
    "\n",
    "    def get_model(self, id: str) -> BaseModel:\n",
    "        if id in ANTHROPIC_MODEL_IDS:\n",
    "            return AnthropicModel(id)\n",
    "        elif id in GOOGLE_MODEL_IDS:\n",
    "            return GoogleModel(id)\n",
    "        elif id in HUGGINGFACE_ENCODER_DECODER_MODEL_IDS:\n",
    "            return HuggingFaceEncoderDecoderModel(id)\n",
    "        elif id in HUGGINGFACE_DECODER_MODEL_IDS:\n",
    "            return HuggingFaceDecoderModel(id)\n",
    "        elif id in OPENAI_MODEL_IDS:\n",
    "            return OpenAIModel(id)\n",
    "        else:\n",
    "            raise ValueError(\"invalid model\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"<{self.__class__.__name__}()>\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a353cda7-1a96-436b-841f-f469e18b79b7",
   "metadata": {},
   "source": [
    "## 1.2 Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ac20a80-5579-4fb5-b971-0fbe54485cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "factory = ModelFactory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94ad6a5f-cd48-404b-b55b-7e3a0cf23f54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anthropic/claude-v1',\n",
       " 'anthropic/claude-v1-100k',\n",
       " 'anthropic/claude-instant-v1',\n",
       " 'anthropic/claude-instant-v1-100k',\n",
       " 'google/text-bison-001',\n",
       " 'google/chat-bison-001',\n",
       " 'google/t5-small',\n",
       " 'google/t5-base',\n",
       " 'google/t5-large',\n",
       " 'google/t5-3b',\n",
       " 'google/t5-11b',\n",
       " 'google/flan-t5-small',\n",
       " 'google/flan-t5-base',\n",
       " 'google/flan-t5-large',\n",
       " 'google/flan-t5-xl',\n",
       " 'google/flan-t5-xxl',\n",
       " 'openai/gpt2',\n",
       " 'bigscience/bloom-560m',\n",
       " 'bigscience/bloom-1b1',\n",
       " 'bigscience/bloom-1b7',\n",
       " 'bigscience/bloom-3b',\n",
       " 'bigscience/bloom-7b1',\n",
       " 'bigscience/bloom',\n",
       " 'bigscience/bloomz-560m',\n",
       " 'bigscience/bloomz-1b1',\n",
       " 'bigscience/bloomz-1b7',\n",
       " 'bigscience/bloomz-3b',\n",
       " 'bigscience/bloomz-7b1',\n",
       " 'bigscience/bloomz',\n",
       " 'facebook/xglm-564M',\n",
       " 'facebook/xglm-1.7B',\n",
       " 'facebook/xglm-2.9B',\n",
       " 'facebook/xglm-4.5B',\n",
       " 'facebook/xglm-7.5B',\n",
       " 'meta-llama/Llama-2-7b-hf',\n",
       " 'meta-llama/Llama-2-7b-chat-hf',\n",
       " 'meta-llama/Llama-2-13b-hf',\n",
       " 'meta-llama/Llama-2-13b-chat-hf',\n",
       " 'meta-llama/Llama-2-70b-hf',\n",
       " 'meta-llama/Llama-2-70b-chat-hf',\n",
       " 'openai/gpt-4',\n",
       " 'openai/gpt-3.5-turbo',\n",
       " 'openai/text-davinci-003',\n",
       " 'openai/text-curie-001',\n",
       " 'openai/text-babbage-001',\n",
       " 'openai/text-ada-001']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factory.list_models()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6448ea5a-523e-4d50-a633-5f149ed26258",
   "metadata": {},
   "source": [
    "### 1.2.1 Anthropic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1f2ef80-9b68-4b80-a30c-bab238520924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The first letter of the alphabet is A.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = factory.get_model(\"anthropic/claude-instant-v1\")\n",
    "model.generate(\"Question: What is the first letter of the alphabet?\\nAnswer: \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0214dd84-db70-48ed-96ea-ee1adf8fbc13",
   "metadata": {},
   "source": [
    "### 1.2.2 Google Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92943c0-b6e4-4832-b77c-da30b19d04b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a95415f-5207-47db-8e2e-8524d2e78353",
   "metadata": {},
   "source": [
    "### 1.2.3 HuggingFace Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c379bbd7-9854-45b7-82a3-fdedafab0bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:A matching Triton is not available, some optimizations will not be enabled.\n",
      "Error caught was: No module named 'triton'\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\xa0I am guessing that the first letter of the alphabet is the first syllable. This means that in English when you make an error, it\\'s easier to fix the mistake.\\nWhat is the first letters of the alphabet?\\nThe first letter of the alphabet is the first syllable. If you use the first syllable of any word and it\\'s a comma, then you will end up with a \"mixed sentence\" (which is what this is) that is, you have to make the mistake of not using the \"mixed sentence\" correctly.\\nWhat is the second letter of the alphabet?\\nThe second letter of the alphabet is the first syllable. When you make an error, it\\'s easier to fix the mistake.\\nWhat is the third letter of the alphabet?\\nThe third letter of the alphabet is the first syllable. When you make an error, it\\'s easier to fix the mistake.\\nWhat is the fourth letter of the alphabet?\\nThe fourth letter of the alphabet is the first syllable. When you make an error, it\\'s easier to fix the error.\\nWhat is the fifth letter of the alphabet?\\nThe fifth letter of the alphabet is the first syllable. When you make an error, it\\'s'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = factory.get_model(\"openai/gpt2\")\n",
    "model.generate(\"Question: What is the first letter of the alphabet?\\nAnswer: \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7c61b1e-5b3b-472b-a190-31b6f7a1317a",
   "metadata": {},
   "source": [
    "### 1.2.4 OpenAI Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb2dc137-acf7-4be8-9e80-9d424fa4bafc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' A'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = factory.get_model(\"openai/text-davinci-003\")\n",
    "model.generate(\"Question: What is the first letter of the alphabet?\\nAnswer: \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
